---
title: "ST 558 Project 1"
author: "John Hinic"
date: '2022-06-26'
output:
  pdf_document:
    toc: true
    toc_depth: 2
    df_print: tibble
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(httr)
library(jsonlite)
```

# Querying APIs in R

## Introduction

The purpose of this vignette is to give a brief overview of accessing APIs within R, writing custom functions that make the process easier, and performing some basic exploratory data analysis (EDA) on the data we use.

The API we are going to utilize is [Polygon.io](https://polygon.io/docs/getting-started), which contains some financial data of various stocks, options, and currencies.


### Required Packages

The following packages (and their purpose) will be required:

- `tidyverse`:
- `httr`:
- `jsonlite`: 


### Basic API Info

The Polygon.io API contains a plethora of financial data pertaining to the stock markets as well as cryptocurrency. As far as pulling actual market data, there are several possible endpoints in the free version of the API:

- Aggregates (Bars)
- Grouped Daily (Bars)
- Daily Open/Close
- Previous Close

There are also many endpoints for reference data, which give more details to inform queries. These reference endpoints contain things like the tickers themselves, the types of tickers, news about tickers, etc.

It is not feasible to explore the entire API in this vignette, so we will focus on the "Aggregates (Bars)" market data endpoint, as well as the "Ticker Details V3" reference data endpoint.

## Accessing the API

Accessing desired parts of the API essentially boils down to constructing the correct URL. To do this, we start with the base URL (`https://api.polygon.io`), then add onto that based on which data we want. As stated, we are going to focus on exploring one market data endpoint and one reference data endpoint. Each of these endpoints has a specific URL format that you will need to add to the end of the base URL, with several parameters for the user to specify within the "{}". The formats are as follows:

- Aggregates (Bars): `/v2/aggs/ticker/{stocksTicker}/range/{multiplier}/{timespan}/{from}/{to}`
  - stocksTicker: the ticker symbol for the stock data we want to pull
  - multiplier: what to multiply the specified timespan by (i.e. if multiplier = 3 and timespan = day, the 3-day bars will be returned)
  - timespan: the size of the time window for the bars (can be minute, hour, day, week, month, quarter, or year)
  - from: the start date in format "YYYY-MM-DD" (up to 2 years prior to the current date)
  - to: the end date in format "YYYY-MM-DD"
- Ticker Details V3: `/v3/reference/tickers/{ticker}`
  - ticker: the ticker symbol we want to pull the details of
  
There are also some options that are not required that the user can specify, such as whether the results should be adjusted for splits (`TRUE` by default), whether the sort should be ascending or descending, and a limit for the number of results. To specify these options, you would construct the URL as previously described, put a '?' at the end, and then specify the desired options (separated by '&').

For any query of the API, you will also need to supply your API key at the end (similar to other options, you would specify 'apiKey=*yourkey*').

We are going to write some custom functions that allow the user to specify which data they want, and will then construct the URL, parse the data, and return a neat dataframe for the user to analyze.

### Writing Functions

The first function will be designed to query the bars endpoint. It will require several arguments from the user:

- ticker (can be the symbol for any company or the full company name for a select few)
- start/end dates (must be in YYYY-MM-DD format)
- multiplier (as described above)
- timespan (as described above - minute, hour, day, week, month, quarter, or year)
- apikey (the user's API key)

As stated, the ticker can be the official ticker symbol (i.e. "AAPL" for Apple) for any company, but will accept the following company names as well:

- "Apple", "Apple Inc."
- "Google", "Alphabet Inc. Class A"
- "Microsoft", "Microsoft Corporation"
- "Amazon", "Amazon.com, Inc."

Aside from the required arguments, there will also be several options that the user can specify if desired:

- adjusted (boolean, whether the results should be adjusted for splits)
- sort ("asc"/"ascending" or "desc"/"descending")
- limit (the maximum number of base aggregates used to create aggregate results)

The returned data frame will only contain the relevant columns, not the data pertaining to the query itself.

```{r bars function}
pullBars <- function(
    ticker, 
    start, 
    end, 
    multiplier = 1, 
    timespan = "day", 
    adj = TRUE, 
    sort = "asc", 
    limit = 50000,
    apikey = "meDW5a0CdiQrg_pqfIRI1yanOQ3FwPFu"
) {
  # resolving ticker symbols
  tick <- switch(
    tolower(ticker),
    "apple" = ,
    "apple inc." = "AAPL",
    "google" = ,
    "alphabet class a inc." = "GOOGL",
    "amazon" = ,
    "amazon.com, inc." = "AMZN",
    "microsoft" = ,
    "microsoft corporation" = "MSFT",
    ticker
  )
  base <- "https://api.polygon.io/v2/aggs/ticker/"
  #/v2/aggs/ticker/{stocksTicker}/range/{multiplier}/{timespan}/{from}/{to}
  base2 <- paste0(base, tick, '/range/', as.character(multiplier), '/', timespan, '/', start, '/', end, '/')
  
  # resolving sort values
  srt <- switch(
    tolower(sort),
    "asc" = ,
    "ascending" = "sort=asc",
    "desc" = ,
    "descending" = "sort=desc"
  )
  
  # resolving adjusted option
  adjOpt <- switch(
    as.character(adj),
    "TRUE" = "adjusted=true",
    "FALSE" = "adjusted=false"
  )
  
  # resolving limit option
  lim <- paste0("limit=", as.character(limit))
  
  # resolving key
  key <- paste0("apiKey=", apikey)
  
  # combining options into single string
  opts <- paste(srt, adjOpt, lim, key, sep = "&")
  
  # combining base URL with options for final URL
  url <- paste(base2, opts, sep = "?")
  
  # querying data and converting to data frame with only relevant columns
  GET(url) %>%
    content("text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE) %>%
    as.data.frame() %>%
    select(starts_with("ticker"), queryCount, resultsCount, adjusted, volume = results.v, 
           volWeighted = results.vw, open = results.o, close = results.c, 
           high = results.h, low = results.l, transactions = results.n)
}
```

With the function to pull the actual market data complete, we will now construct a function that pulls reference data with specific details about the supplied ticker and overall company. This function will be similar to the previous, but have fewer arguments due to the lower level of complexity:

- ticker (identical to previous function)
- date (optional, will return information about the supplied ticker on the specific date. Must be in YYYY-MM-DD format)
- apikey (the user's API key)

This function will return 

```{r details function}
pullDetails <- function(
    ticker,
    date = '',
    apikey = 'meDW5a0CdiQrg_pqfIRI1yanOQ3FwPFu'
) {
  # resolving ticker symbols
  tick <- switch(
    tolower(ticker),
    "apple" = ,
    "apple inc." = "AAPL",
    "google" = ,
    "alphabet class a inc." = "GOOGL",
    "amazon" = ,
    "amazon.com, inc." = "AMZN",
    "microsoft" = ,
    "microsoft corporation" = "MSFT",
    ticker
  )
  
  # resolving key
  key <- paste0("apiKey=", apikey)
  
  # resolving date if specified
  if(length(date) > 0){
    dateOpt <- paste0('date=', date, '&')
  } else {
    dateOpt <- ''
  }
  
  # constructing final URL
  base <- 'https://api.polygon.io/v3/reference/tickers/'
  opts <- paste0(tick, '?', dateOpt, key)
  url <- paste0(base, opts)
  
  # querying data and converting to data frame
  GET(url) %>%
    content("text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE) %>%
    as.data.frame()
}
```


### Function Demonstration

Now that both functions are completed, we will demonstrate some basic usage of them. For instance, we can pull daily aggregate (adjusted) data about Google from the beginning of 2022 until the start of June:

```{r bars1}
pullBars("Google", "2022-01-01", "2022-06-01")
```

Notice that despite the many possible arguments of the function, we can simply specify the ticker, start date, and end date and it will return the data with reasonable defaults. We can also pull the company details using the second function (this time using the actual ticker symbol):

```{r details1}
pullDetails("GOOGL")
```

Or, we could pull details about Google specifically from the first day of 2022:

```{r details2}
pullDetails("GOOGL", "2022-01-01")
```

Because the `results.description` column is empty here, that means there were no details regarding GOOGL from that day.

We could also get a bit more complicated with the market data we pull. For example, we can pull bi-weekly aggregate (un-adjusted) data about Microsoft from the same time frame:

```{r bars2}
pullBars("Microsoft", "2022-01-01", "2022-06-01", multiplier = 2, timespan = "week", adj = FALSE)
```

### Analysis Data

Now that we have written and tested our functions, we are going to use them to pull some data for basic exploratory data analysis. We are going to pull the weekly aggregate (adjusted) data of Apple, Microsoft, Google, and Amazon going from all of 2021, then concatenate them all into a single dataset with an added variable called "week" that is simply an indicator for what number week it is in the year.

```{r analysis data}
apple <- pullBars("Apple", "2021-01-01", "2021-12-31", timespan = "week")
google <- pullBars("Google", "2021-01-01", "2021-12-31", timespan = "week")
msft <- pullBars("Microsoft", "2021-01-01", "2021-12-31", timespan = "week")
amazon <- pullBars("Amazon", "2021-01-01", "2021-12-31", timespan = "week")
all <- rbind(apple, google, msft, amazon) %>%
  mutate(week = rep(seq(1:53), 4))
```

We are also going to derive another variable of interest. We are going to subtract the open price from the closing price, then take that as a proportion of the weighted average (and multiply that number by 100 to make the numbers more readable). This will give us a measure of growth that is proportional to the overall price of the stock for each company.

```{r final data}
final <- all %>%
  mutate(growth = 100*(close - open) / volWeighted)
```

## Exploratory Data Analysis

For our exploratory data analysis, we are primarily going to look at the previously derived `growth` variable  as our response. We are going to look at this across the different companies, as well as weeks of the year.

### Univariate Summaries

To start, we will look at some simple summaries of our growth variable.

```{r growth summary}
summary(final$growth)
```

We can see that our mean is slightly higher that our median, which means the data might be a little right skewed. The fact that both measures of center are above 0 indicates that, on average, all 4 companies had positive growth over 2021. To further investigate, we will look at a histogram:

```{r growth hist}
final %>% ggplot(aes(growth)) +
  geom_histogram(bins = 20) +
  labs(title = "Histogram of Growth Rate", x = "Growth Rate") +
  theme_minimal()
```

It appears that the data is very slightly right skewed, but is pretty symmetrical for the most part.

### Bivariate Summaries

To start with, we will look at summaries of the growth rate across the 4 companies.

```{r summary by company}
final %>%
  group_by(ticker) %>%
  summarise(Median = median(growth),
            Mean = mean(growth),
            StD = sd(growth),
            Min = min(growth),
            Max = max(growth))
```

Here, it looks like 3 of the 4 companies had a positive growth rate, but Amazon actually had negative median growth rate throughout 2021. However, the mean is still above 0 - this would mean that Amazon had more negative growth weeks than positive, but overall the positive growth weeks slightly outweighed the negative.

We can also look at the histograms grouped by company (with a reference line at 0):

```{r histogram by company}
final %>% ggplot(aes(growth, fill = ticker)) +
  geom_histogram(bins = 20) +
  labs(title = "Histograms of Growth Rate by Company",x = "Growth Rate") +
  theme_minimal() +
  geom_vline(xintercept = 0) +
  facet_wrap(~ticker)
```

Here, it looks like Microsoft and Google clearly had better years than Apple and Amazon. It appears that Apple had several very good weeks in terms of growth but was largely centered at 0, while Amazon had no particularly good weeks but had 1 particularly bad one. On the other hand, Google and Microsoft appear to have been very consistently high.

Similarly, we can look at box plots as well:

```{r boxplot by company}
final %>% ggplot(aes(x = ticker, y = growth)) +
  geom_boxplot() +
  labs(title = "Box Plots of Growth Rate by Company", x = "Growth Rate") +
  theme_minimal()
```

Here we can clearly see the few outlying months for Apple (2 of positive growth and 1 with negative), as well as several very positive months for Google. On the other hand, we can see that Microsoft was just consistently high, while Amazon was consistently low (relative to the others).

### Trivariate Summaries

Instead of looking at simple summaries of the growth rate, we can also look at the weighted selling averages across time (i.e. the weeks). To start, we will look at a simple line graph of week by weighted average, colored by company.

```{r line plot}
final %>% ggplot(aes(week, volWeighted, color = ticker)) + 
  geom_line() +
  facet_wrap(~ticker, scales = "free_y") +
  labs(title = "Average Price over Time by Company", y = "Weighted Average") +
  theme_minimal()
```

When looking at the prices over time, we could easily form a different opinion about the Apple stock prices. Clearly, they had a dip in the middle of the year but started to rapidly increase towards the end of the year. This means that the 2 particularly high values of growth rate for Apple were likely at the end of the year, which would be much more relevant in predicting the next years growth compared to earlier numbers.

It also appears that Google's growth might have been plateauing somewhat towards the end of the year, but it is hard to say without further analysis. Microsoft seems to consistently dip and then continue rising, while Amazon seems to be all over the place.

Because of this trend at the end of the year specifically for Apple and Google, we can look at a contingency table of number of transactions by week for those two companies as well.

```{r contingency table}
final %>% filter(ticker %in% c("GOOGL", "AAPL")) %>%
  mutate(adjCount = transactions / 100000) %>%
  group_by(ticker, week) %>%
  summarise(n = adjCount) %>%
  pivot_wider(names_from = ticker, values_from = n) %>%
  print(n = 53)
```

Here, we can see that both transaction numbers are highly variable, with Google's being much higher in general. It will be much easier to digest this data looking at 2 bar graphs:

```{r bar graph}
final %>% filter(ticker %in% c("GOOGL", "AAPL")) %>%
  mutate(adjCount = transactions / 100000) %>%
  ggplot(aes(x = week, y = adjCount, fill = ticker)) +
  geom_bar(stat="identity") +
  facet_wrap(~ticker, scales = "free_y") +
  theme_bw() +
  labs(title = "Number of Transactions by Week", y = "Number of Transactions (in 100,000's)")
```

Here, it definitely does appear that Apple has 2 weeks that stand out towards the end of the year, but they are still not as high as some weeks early in the year. On the other hand, Google's transaction numbers seems to be highly variable all throughout the year. 

Lastly, we can look at a scatter plot of the opening vs. closing prices colored by company. We expect them to be closely correlated, but looking at the scatter plot by company may make the patterns a bit more clear.

```{r scatter plot}
final %>% ggplot(aes(open, close, color = ticker)) + 
  geom_point() +
  facet_wrap(~ticker, scales = "free") +
  labs(title = "Opening vs. Closing Prices by Company", y = "Closing Price", x = "Opening Price") +
  theme_minimal()
```

As expected, the two values are very closely correlated. However, the patterns do seem to vary a decent bit by company. Apple has the few outlier weeks very high for both, while Google seems to have some clustering on both ends (that are also a bit more varied). Amazon is much more spread out in general, whioe Microsoft appears to be the king of consistency.